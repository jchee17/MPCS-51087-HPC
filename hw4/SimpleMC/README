1. I set stream, seed0, and seed to threadprivate. This is to ensure that there are no write conflicts during the random number generations between threads. I made tallies, source bank, and fission bank global. I parallelized the loop over particles in run_eigenvalue() in eigenvalue.c. I locked sections in collision() and score_tally() using omp critical so that only one thread could write to the fission bank or tally datastructures at a time. 
Included in the file data is a comparison of the keff values generated from the serial code and the keff values generated from openmp parallelized code with 8 threads. The keff scores are not exactly the same because the random numbers are generated differently between the serial and parllel versions because each thread holds its own private copy of stream, seed0, and seed. We can clearly see that the keff values for both the serial and 8thread openmp parallel code both converge to the same value, 1.


2. Look at Scaling_study.jpg  I compare the performance of static and dynamic scheduling for 1 to 32 threads. Both static and dynamic scheduling plateaued to around 7 second runtime for 16 and 32 threads. This plateau occurs because we cannot get anymore speedup from strong scaling for this problem size, there is not enough work for each thread (when we have over 16 threads) to amortize the coast of the thread overhead. 
The static scheduling perfromed better than the dynamic scheduling on number of threads less than 16. Because of the high number of particles and relative uniform time in computation, it appears that static scheduling was more effective and the extra cost that went into dynamic scheduling was not worth it because there was not a high variation in the computations for particles. 
Both static and dyanmic scheduling plateaued to the same time for computation, indicating that any time difference between the scheduling modes becomes insignificant once there are too many threads to be useful for the problem size.
The best overall time was achieved by static scheduling with 8 threads. This appears to be the optimal balance between the number of threads and the amount of work per thread.  
The exact parameters of the scaling study can be found in the file data. There were 1,000,000 particles, 10 batches, 10 active batches, and 1 generation per batch.
